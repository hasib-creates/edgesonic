#!/usr/bin/env python3
"""
Compare golden CSV outputs against Android (or other) validation runs.

Usage example:
    python validation/compare_runs.py \
        --golden validation/golden \
        --candidate /tmp/android_validation \
        --tolerance 5e-3
"""
from __future__ import annotations

import argparse
import csv
import json
import statistics
from pathlib import Path
from typing import Dict, List, Sequence, Tuple


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Compare per-window anomaly scores between golden CSVs and candidate outputs."
    )
    parser.add_argument(
        "--golden",
        type=Path,
        required=True,
        help="Directory containing the golden CSV files generated by generate_golden.py.",
    )
    parser.add_argument(
        "--candidate",
        type=Path,
        required=True,
        help="Directory containing the candidate CSV files pulled from Android.",
    )
    parser.add_argument(
        "--tolerance",
        type=float,
        default=5e-3,
        help="Maximum allowed absolute difference per window (default: 5e-3).",
    )
    parser.add_argument(
        "--report",
        type=Path,
        help="Optional path to write a JSON report summarising the comparison.",
    )
    return parser.parse_args()


def load_csv_rows(path: Path) -> List[Dict[str, float]]:
    """Load per-window rows from a CSV, ignoring header comments."""
    with path.open("r", newline="") as handle:
        data_lines = [line for line in handle if not line.startswith("#")]
    reader = csv.DictReader(data_lines)

    rows: List[Dict[str, float]] = []
    for raw_row in reader:
        rows.append(
            {
                "window_index": int(raw_row["window_index"]),
                "time_sec": float(raw_row["time_sec"]),
                "raw_score": float(raw_row["raw_score"]),
                "smoothed_score": float(raw_row["smoothed_score"]),
                "threshold": float(raw_row["threshold"]),
                "is_anomaly": int(raw_row.get("is_anomaly", 0)),
            }
        )
    return rows


def summarise_diffs(
    golden_rows: Sequence[Dict[str, float]], candidate_rows: Sequence[Dict[str, float]]
) -> Tuple[List[Dict[str, float]], Dict[str, float]]:
    golden_map = {row["window_index"]: row for row in golden_rows}
    candidate_map = {row["window_index"]: row for row in candidate_rows}

    shared_indices = sorted(set(golden_map) & set(candidate_map))
    missing_in_candidate = sorted(set(golden_map) - set(candidate_map))
    missing_in_golden = sorted(set(candidate_map) - set(golden_map))

    diffs: List[Dict[str, float]] = []
    abs_raw: List[float] = []
    abs_smoothed: List[float] = []
    for idx in shared_indices:
        g = golden_map[idx]
        c = candidate_map[idx]
        diff_raw = abs(g["raw_score"] - c["raw_score"])
        diff_smooth = abs(g["smoothed_score"] - c["smoothed_score"])
        diffs.append(
            {
                "window_index": idx,
                "time_sec_golden": g["time_sec"],
                "time_sec_candidate": c["time_sec"],
                "raw_abs_diff": diff_raw,
                "smoothed_abs_diff": diff_smooth,
            }
        )
        abs_raw.append(diff_raw)
        abs_smoothed.append(diff_smooth)

    summary = {
        "shared_windows": len(shared_indices),
        "missing_in_candidate": missing_in_candidate,
        "missing_in_golden": missing_in_golden,
        "max_abs_diff_raw": float(max(abs_raw, default=0.0)),
        "max_abs_diff_smoothed": float(max(abs_smoothed, default=0.0)),
        "mean_abs_diff_raw": float(statistics.fmean(abs_raw)) if abs_raw else 0.0,
        "mean_abs_diff_smoothed": float(statistics.fmean(abs_smoothed)) if abs_smoothed else 0.0,
    }

    return diffs, summary


def collect_csv_pairs(golden_dir: Path, candidate_dir: Path) -> List[Tuple[Path, Path]]:
    pairs: List[Tuple[Path, Path]] = []
    for golden_csv in sorted(golden_dir.glob("*.csv")):
        candidate_csv = candidate_dir / golden_csv.name
        if candidate_csv.exists():
            pairs.append((golden_csv, candidate_csv))
        else:
            print(f"⚠️  Missing candidate file for {golden_csv.name}")
    return pairs


def main() -> None:
    args = parse_args()
    if not args.golden.exists():
        raise FileNotFoundError(f"Golden directory not found: {args.golden}")
    if not args.candidate.exists():
        raise FileNotFoundError(f"Candidate directory not found: {args.candidate}")

    csv_pairs = collect_csv_pairs(args.golden, args.candidate)
    if not csv_pairs:
        raise SystemExit("No CSV pairs found to compare.")

    report_entries = []
    failures = []

    for golden_csv, candidate_csv in csv_pairs:
        golden_rows = load_csv_rows(golden_csv)
        candidate_rows = load_csv_rows(candidate_csv)
        diffs, summary = summarise_diffs(golden_rows, candidate_rows)

        over_tolerance = [
            diff
            for diff in diffs
            if diff["smoothed_abs_diff"] > args.tolerance or diff["raw_abs_diff"] > args.tolerance
        ]

        print(f"\nComparing {golden_csv.name}:")
        print(
            f"  windows={summary['shared_windows']}, "
            f"max|Δraw|={summary['max_abs_diff_raw']:.6f}, "
            f"max|Δsmooth|={summary['max_abs_diff_smoothed']:.6f}"
        )
        if summary["missing_in_candidate"]:
            print(f"  Missing windows in candidate: {summary['missing_in_candidate']}")
        if summary["missing_in_golden"]:
            print(f"  Extra windows in candidate: {summary['missing_in_golden']}")
        if over_tolerance:
            print(f"  ❌ {len(over_tolerance)} windows exceed tolerance {args.tolerance}")
            failures.append(
                {
                    "file": golden_csv.name,
                    "exceeding_windows": over_tolerance,
                    "summary": summary,
                }
            )
        else:
            print("  ✅ Differences within tolerance.")

        report_entries.append(
            {
                "file": golden_csv.name,
                "summary": summary,
                "over_tolerance": over_tolerance,
            }
        )

    if args.report:
        args.report.parent.mkdir(parents=True, exist_ok=True)
        with args.report.open("w", encoding="utf-8") as handle:
            json.dump(
                {
                    "tolerance": args.tolerance,
                    "results": report_entries,
                    "failures": failures,
                },
                handle,
                indent=2,
            )
        print(f"\nSaved report to {args.report}")

    if failures:
        raise SystemExit(1)
    else:
        print("\nAll files matched within tolerance.")


if __name__ == "__main__":
    main()
